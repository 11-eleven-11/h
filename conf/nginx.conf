daemon off;
worker_processes auto;
pid /var/lib/hypothesis/nginx.pid;
error_log /dev/stderr;

events {
  worker_connections 4096;
}

http {
  client_max_body_size 20m;
  sendfile on;
  server_tokens off;

  include mime.types;
  default_type application/octet-stream;

  access_log off;

  # If there is an auth token, rate limit based on that,
  # otherwise rate limit per ip.
  map $http_authorization $limit_per_user {
    "" $binary_remote_addr;
    default $http_authorization;
  }

  # 1m stands for 1 megabyte so the zone can store ~8k users.
  # User's typically don't go over 1rps including bots so set the
  # generic rate limit of all endpoints to 1rps.
  limit_req_zone $limit_per_user zone=user_1rps_limit:1m rate=1r/s;
  
  # We set fail_timeout=0 so that the upstream isn't marked as down if a single
  # request fails (e.g. if gunicorn kills a worker for taking too long to handle
  # a single request).
  upstream web { server unix:/tmp/gunicorn-web.sock fail_timeout=0; }
  upstream websocket { server unix:/tmp/gunicorn-websocket.sock fail_timeout=0; }

  server {
    listen 5000;

    server_name _;
    server_tokens off;

    root /var/www;
    
    location /ws {
      proxy_pass http://websocket;
      proxy_http_version 1.1;
      proxy_redirect off;
      proxy_buffering off;
      proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection upgrade;
      proxy_set_header Host $host;
      proxy_set_header X-Forwarded-Server $http_host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    location /annotating-all-knowledge/ {
      proxy_pass https://hypothesis.github.io;
      proxy_http_version 1.1;
      proxy_set_header Host $proxy_host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    location /roadmap {
      return 302 "https://trello.com/b/2ajZ2dWe/public-roadmap";
    }

    location / {
      # A bot may burst up to 50rpm so and the client issues 5 requests upon
      # loading the sidebar. Assume a max request rate of 15 rps in a burst.
      # This means the queue size would be 14 requests.
      # Send up to 15 requests directly to the server but only allow
      # 1 new request each second after that.
      limit_req zone=user_1rps_limit burst=14 nodelay;
      limit_req_status 429;

    # Each /api/annotations:create request has a 95 percentile 
    # response time of .56s and there are 12 gunicorn
    # workers per host. Create requests account for <1% of the 
    # traffic on the host so assume .12 workers are allocated
    # for /api/annotations:create requests.
    #   .12 workers * 1 request / .56 s = .21 requests/s = ~1rps
    # If too many of these requests happen back to back it can
    # overwhelm the database so instead of letting a burst of
    # requests pass to the server, queue these requests and 
    # only send 1 each second.
    # Allow a user to queue up 4 rps (4 times the expected rate). 
    location /api/annotations {
      limit_req zone=user_1rps_limit burst=3;
      limit_req_status 429;

      proxy_pass http://web;
    }

      proxy_pass http://web;
      proxy_http_version 1.1;
      proxy_connect_timeout 10s;
      proxy_send_timeout 10s;
      proxy_read_timeout 10s;
      proxy_redirect off;
      proxy_set_header Host $host;
      proxy_set_header X-Forwarded-Server $http_host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Request-Start "t=${msec}";
    }
  }

  server {
    listen 127.0.0.234:5000;
    server_name _;

    location /status {
      stub_status on;
      access_log off;
      allow 127.0.0.0/24;
      deny all;
    }
  }
}
