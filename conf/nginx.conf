daemon off;
worker_processes auto;
pid /var/lib/hypothesis/nginx.pid;
error_log /dev/stderr;

events {
  worker_connections 4096;
}

http {
  client_max_body_size 20m;
  sendfile on;
  server_tokens off;

  include mime.types;
  default_type application/octet-stream;

  access_log off;

  # If there is an auth token, rate limit based on that,
  # otherwise rate limit per ip.
  map $http_authorization $limit_per_user {
    "" $binary_remote_addr;
    default $http_authorization;
  }

  # 1m stands for 1 megabyte so the zone can store ~8k users.
  # User's typically don't go over 1rps including bots so set the
  # generic rate limit of all endpoints to 1rps.
  limit_req_zone $limit_per_user zone=user_1rps_limit:1m rate=1r/s;

  # We set fail_timeout=0 so that the upstream isn't marked as down if a single
  # request fails (e.g. if gunicorn kills a worker for taking too long to handle
  # a single request).
  upstream web { server unix:/tmp/gunicorn-web.sock fail_timeout=0; }
  upstream websocket { server unix:/tmp/gunicorn-websocket.sock fail_timeout=0; }

  server {
    listen 5000;

    server_name _;
    server_tokens off;

    root /var/www;

    location /ws {
      proxy_pass http://websocket;
      proxy_http_version 1.1;
      proxy_redirect off;
      proxy_buffering off;
      proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection upgrade;
      proxy_set_header Host $host;
      proxy_set_header X-Forwarded-Server $http_host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    location /annotating-all-knowledge/ {
      proxy_pass https://hypothesis.github.io;
      proxy_http_version 1.1;
      proxy_set_header Host $proxy_host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    location /roadmap {
      return 302 "https://trello.com/b/2ajZ2dWe/public-roadmap";
    }

    location / {
      # A bot may burst up to 50rpm and the client issues 5 requests upon
      # loading the sidebar. Assume a max request rate of 15 rps in a burst.
      # This means the queue size would be 14 requests.
      # Allow a user to sustain the max bursty request rate for 3 seconds.
	  # This would mean they can request up to 45 requests in one second but
      # only 1 new request each second after that.
      limit_req zone=user_1rps_limit burst=44 nodelay;
      limit_req_status 429;

      # The 95th percentile time for a badge request is .042s.
      # 7.6% of worker time is spent handling these requests.
      # Typical usage per user is around 50 rpm.
      # Queue up badge requests rather than sending them directly
      # to the server-this will allow other requests to take priority.
      location /api/badge {
        limit_req zone=user_1rps_limit burst=15;

        proxy_pass http://web;
      }

      # The 95th percentile time for an asset is .013s.
      # The maximum burst of requests from a single page
      # https://hypothes.is/docs/help is 20 requests.
      # <1% of worker time is spent handling these requests.
      # The maximum expected request rate is 25rpm.
      # Assume in frustration the user hammers on the refresh
      # button 7 times in a row. Worst case this results in
      # a burst of 140 requests.
      location /assets {
        limit_req zone=user_1rps_limit burst=139 nodelay;

        proxy_pass http://web;
      }

      # Each /api/annotations:create request has a 95 percentile
      # response time of .56s and there are 12 gunicorn
      # workers per host. Create requests account for <1% of the
      # traffic on the host so assume .12 workers are allocated
      # for /api/annotations:create requests.
      #   .12 workers * 1 request / .56 s = .21 requests/s = ~1rps
      # If too many of these requests happen back to back it can
      # overwhelm the database so instead of letting a burst of
      # requests pass to the server, queue these requests and
      # only send 1 each second.
      # Allow a user to queue up 8 rps (8 times the expected rate).
      location =/api/annotations {
        limit_req zone=user_1rps_limit burst=8;

        proxy_pass http://web;
      }

      proxy_pass http://web;
      proxy_http_version 1.1;
      proxy_connect_timeout 10s;
      proxy_send_timeout 10s;
      proxy_read_timeout 10s;
      proxy_redirect off;
      proxy_set_header Host $host;
      proxy_set_header X-Forwarded-Server $http_host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Request-Start "t=${msec}";
    }
  }

  server {
    listen 127.0.0.234:5000;
    server_name _;

    location /status {
      stub_status on;
      access_log off;
      allow 127.0.0.0/24;
      deny all;
    }
  }
}
